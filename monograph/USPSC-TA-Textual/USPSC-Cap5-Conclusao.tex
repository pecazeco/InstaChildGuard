%!TEX root = tese/main.tex
\chapter{Conclusões e possíveis melhorias} \label{chap:conclusao}

\section{Conclusões da prova de conceito}
Por fim, utilizando o Llama 4 Scout foi possível atingir 95\% de acurácia em tempo médio de menos de 1{,}5 segundos por imagem.
Conclui-se, portanto, que é possível implementar um modelo multimodal para identificar postagens que potencialmente sexualizam crianças de maneira bastante ágil e com boa acurácia. 

A implementação de um frontend para a extensão para o Instagram também provou que é possível integrar esse modelo de maneira fluida com uma página dinâmica, que sofre mutações o tempo todo.

Além disso, mesmo com os \textit{guard rails} (políticas que guiam e restringem o comportamento de modelos de IA para assegurar que suas saídas sejam seguras, éticas e confiáveis) intrínsicos ao modelo, foi possível contornar os possíveis erros. Isso se deu construindo prompts com jargão técnico, flexibilizando as configurações de segurança do agente e considerando específicos tipos de erro como uma resposta ``sim''.

\section{Melhorias de hipotética implementação na prática}
Em um possível uso dessa tecnologia em escala bem maior e integrado diretamente ao Instagram, o funcionamento provavelmente seria bem diferente.
Há de se destacar que a Meta é a empresa por trás do Instagram e também dos modelos Llama, logo, eles têm bem mais liberdade para fazer um agente Llama treinado especificamente para a tarefa de identificação de sexualização infantil e mais integrado à plataforma, o que melhoraria ainda mais o seu desempenho.

Algumas evoluções que poderiam ser feitas por parte do Instagram em uma possível implementação da ideia:
\begin{itemize}
    \item Além de analisar a potencialidade de sexualização da postagem, também analisar a potencialidade de pedofilia do usuário que está vendo a imagem com base no seu algoritmo. A ideia é afastar conteúdos sugestivos de pessoas que enxergam teor sexual e os mostrar para pessoas que não veem esse teor.
    \item Ao invés de analisar a imagem em tempo real enquanto o usuário acessa, passar a analisar a imagem uma única vez quando ela for postada, economizando recursos de processamento.
    \item Na prática, se o conteúdo for classificado como inadequado a ser mostrado para uma determinada pessoa, essa postagem não deveria nem aparecer, o que elimina o sentido de ter um frontend como feito nesse projeto.
    \item Além de analisar imagens, obviamente o modelo treinado deve ser capaz também para analisar vídeos (stories e reels).
\end{itemize} 

Essas e outras mudanças não só tornariam o método como um todo mais robusto e otimizado, mas também muito mais imperceptível no funcionamento do dia-a-dia do que a forma que a extensão dessa pesquisa foi projetada.

Vale observar que, sendo uma empresa fechada, não sabe-se exatamente que medidas são tomadas pelo Instagram acerca do assunto. 
Com base, por exemplo, no aviso da \autoref{fig:aviso_abuso}, assume-se que a rede social está realizando algum esforço para evitar a busca por conteúdo pedófilo. Porém, essa pesquisa demonstra que é possível fazer ainda mais, dificultando que conteúdos de crianças sejam vistos por pessoas que os veêm de maneira deturpada. 